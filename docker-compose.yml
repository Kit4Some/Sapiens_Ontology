# =============================================================================
# Docker Compose for Ontology Reasoning System
# Full stack: Neo4j + LangGraph Server + API Backend
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # Neo4j Database with APOC plugin
  # ---------------------------------------------------------------------------
  neo4j:
    image: neo4j:5.15.0-community
    container_name: ontology-neo4j
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    environment:
      - NEO4J_AUTH=neo4j/password123
      - NEO4J_PLUGINS=["apoc"]
      # Memory configuration
      - NEO4J_dbms_memory_heap_initial__size=512m
      - NEO4J_dbms_memory_heap_max__size=2G
      - NEO4J_dbms_memory_pagecache_size=512m
      # Security
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
      - NEO4J_dbms_security_procedures_allowlist=apoc.*
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_plugins:/plugins
    networks:
      - ontology-network
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:7474 || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 60s
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # API Backend (FastAPI)
  # ---------------------------------------------------------------------------
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ontology-api
    ports:
      - "8000:8000"
    environment:
      # Application
      - APP_NAME=Ontology Reasoning System
      - APP_VERSION=1.0.0
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
      # Neo4j
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=password123
      - NEO4J_DATABASE=neo4j
      # LLM (set via .env or secrets)
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - LLM_OPENAI_API_KEY=${LLM_OPENAI_API_KEY}
      - LLM_REASONING_MODEL=${LLM_REASONING_MODEL:-gpt-4o-mini}
      - LLM_EMBEDDING_MODEL=${LLM_EMBEDDING_MODEL:-text-embedding-3-small}
      # ToG Settings
      - TOG_MAX_REASONING_DEPTH=5
      - TOG_CONFIDENCE_THRESHOLD=0.7
      # API
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - API_DEBUG=false
    depends_on:
      neo4j:
        condition: service_healthy
    networks:
      - ontology-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # LangGraph Server (ToG 3.0 MACER Workflow)
  # ---------------------------------------------------------------------------
  langgraph:
    build:
      context: .
      dockerfile: Dockerfile.langgraph
    container_name: ontology-langgraph
    ports:
      - "8123:8000"  # LangGraph Server API
    environment:
      # Neo4j
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=password123
      - NEO4J_DATABASE=neo4j
      # LLM (set via .env or secrets)
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - LLM_OPENAI_API_KEY=${LLM_OPENAI_API_KEY}
      - OPENAI_API_KEY=${LLM_OPENAI_API_KEY}
      - LLM_REASONING_MODEL=${LLM_REASONING_MODEL:-gpt-4o-mini}
      - LLM_EMBEDDING_MODEL=${LLM_EMBEDDING_MODEL:-text-embedding-3-small}
      # ToG Settings
      - TOG_MAX_REASONING_DEPTH=5
      - TOG_CONFIDENCE_THRESHOLD=0.75
      # LangGraph
      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2:-false}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY:-}
    depends_on:
      neo4j:
        condition: service_healthy
    networks:
      - ontology-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Redis (Optional - for caching and rate limiting)
  # ---------------------------------------------------------------------------
  redis:
    image: redis:7-alpine
    container_name: ontology-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - ontology-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    profiles:
      - with-cache

  # ---------------------------------------------------------------------------
  # PostgreSQL (for LangGraph checkpointing and telemetry)
  # ---------------------------------------------------------------------------
  postgres:
    image: postgres:16-alpine
    container_name: ontology-postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=ontology
      - POSTGRES_PASSWORD=ontology_pass
      - POSTGRES_DB=ontology_checkpoints
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - ontology-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ontology -d ontology_checkpoints"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    profiles:
      - with-checkpointing

# =============================================================================
# Networks
# =============================================================================
networks:
  ontology-network:
    driver: bridge

# =============================================================================
# Volumes
# =============================================================================
volumes:
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
  neo4j_plugins:
    driver: local
  redis_data:
    driver: local
  postgres_data:
    driver: local
